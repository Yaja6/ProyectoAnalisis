# Proyecto de An√°lisis de Datos

### Link de One Drive con Datasets, Scripts: https://epnecuador-my.sharepoint.com/personal/jonathan_armas_epn_edu_ec/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fjonathan_armas_epn_edu_ec%2FDocuments%2FDataAnalisis&originalPath=aHR0cHM6Ly9lcG5lY3VhZG9yLW15LnNoYXJlcG9pbnQuY29tLzpmOi9nL3BlcnNvbmFsL2pvbmF0aGFuX2FybWFzX2Vwbl9lZHVfZWMvRXZuNXhTRXBaU1pLdnd5aExpRjN5dlVCd3p0ZVhCRjBjSFJIRW5XQnp4d0VYQT9ydGltZT1wMUJ1aFlsVDJFZw

**archivos JSON y CSV Finales:** https://mega.nz/file/vBEjTSpZ#iZidFwpiJi9ryKcBk8Eymwy0bNcU2eIo6_yLORXcbPg

Proceso detallado sobre el uso de scripts

## 1. Cosecha de datos üöÄ

## Pol√≠tica por Ciudades

La referencia _FUENTE_ son los diferentes scripts usados por el equipo, el proceso detallado es similar en los scripts

### Script 1 (FUENTE 1): Politicaxciudad1.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################
ckey = "CuIFG1DblkEC36JFzJByX5mi7"
csecret = "Nc8sgHQ1n2Fb1qpHma9LudER1wOELLP4tNzzpz6YAIQnmRF9Qh"
atoken = "1204786641635827712-XXmfW6V9O2i6CVS4X1SykT5Tp7GpjY"
asecret = "S76F2rIurjJnpwctfYQ9a2tprXp7pP8UNI8lYlfWSuqKW"
#####################################

class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)

auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:couchy@localhost:5984/')
try:
    db = server.create('politica2')
except:
    db = server['politica2']
'''===============LISTAS, NOMBRES Y REPRESENTANTES=============='''
twitterStream.filter(track=['Quito candidatos','Ecuador','Machala','Portoviejo','Esmeraldas','Ciudad de Milagro','Babahoyo','Latacunga','Guillermo Alberto Santiago Lasso Mendoza','Fabricio Correa Delgado','Wilson Gustavo Larrea Cabrera','Lucio Guti√©rrez Borb√∫a','Andr√©s David Ar√°uz Galarza','Guillermo Alejandro Celi Santos','Yaku Sacha P√©rez Guartambel','Cesar Mont√∫far Mancheno','Isidro Romero Carbo','Carlos Gerson Almedia Espinoza','Ximena Pe√±a Pacheco','Pa√∫l Ernesto Carrasco Carpio','Esteban Leopoldo Quirola Bustos','Miguel Salem Kronfle','Cristina Reyes','Xavier Hervas','Pedro Jos√© Freile','Juan Fernando Velasco Torres','Washington Arturo Pes√°ntez Mu√±oz','Uni√≥n Ecuatoriana','Pachakutik','Partido Social Patriota','Consejo Nacional Electoral Ecuador','Centro democr√°tico','Pierina Correa Delgado','Geovanni Atarihuana','Uni√≥n Popular','Ecuador unido','Roc√≠o Juca', 'Partido Social Cristiano','Henry Kronfle', 'Javier Ortiz', 'Libertad es Pueblo','Fernando Balda','Fuerza EC','Abdal√° Bucaram Ortiz', 'Democracia S√≠','Xavier Zavala Egas','C√©sar Monge','Alianza PAIS',  'C√©sar Litardo','Fernando Villavicencio'])

```

**Proceso** 
 En este caso se importaron librer√≠as necesarias como twepy el cual nos permite recolectar los tweets en tiempo  real dentro de scripts de python. 
 Tenemos adem√°s las API KEY generadas desde Twitter Development las cuales nos permiten acceder a la API de twitter para extraer los datos.
 A continuaci√≥n la secci√≥n ==couchdb== se utiliza la direcci√≥n donde se encuentra corriendo CouchDB, que es la Base de datos NO SQL en la que los datos recopilados se a√±adir√°n. Aqu√≠ le indicamos que se debe crear una base de datos llamada _politica2_  
 
Para el filtro de twitter usamos el filtro por palabras debido a que nos permite definir exactamente los temas relacionados a la pol√≠tica. Este apartado contiene aproximadamente 100 palabras como filtro, entre ellos:
 * Nombres de candidatos de las ciudades
 * Nombres de Movimientos y Listas candidatas
 * Nombres de las Ciudades 
 
Una vez explicado el script, se procede a correr el mismo.
Para esto nos ubicamos en la carpeta donde se encuentra el script y abrimos la terminal y ejecutamos el comando: **python Politica1.py**

### Script 2 (FUENTE 2): Pol√≠ticaxciudad2.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json


###API ########################
ckey = "6Zyv4XxVypDqHDpFoHwSTrMzX"
csecret = "3J5TpltHtmEZGEw8RhRLABc3KQ2Quhjj2SVVykfw5zs02fjtpC"
atoken = "153168970-C8H0rPCjztDmLQMrjtgOYSPIzjLMyegrtrAZQQrq"
asecret = "WxWpMOMlghN1tVYZRFugRWTefM1SShLWVI4lL4oPWTAlO"
#####################################

class listener(StreamListener):
    
    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print ("SAVED" + str(doc) +"=>" + str(data))
        except:
            print ("Already exists")
            pass
        return True
    
    def on_error(self, status):
        print (status)
        
auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:admin@localhost:5984/')  #('http://115.146.93.184:5984/')
try:
    db = server.create('presidenciales_nacionales')
except:
    db = server['presidenciales_nacionales']
    
'''===============LOCATIONS=============='''    

#twitterStream.filter(locations=[-79.95912,-2.287573,-79.856351,-2.053362]) 
twitterStream.filter(track=["Andr√©s Arauz","Lucio Guti√©rrez","David Norero","Gerson Almeida","Martha Villafuerte","Cristina Reyes","Diego Salgado","Isidro Romero","Sof√≠a Merino", "Esteban Quirola", "Juan Carlos Machuca","Miguel Salem Kronfle","Gustavo Bucaram Ortiz", "Fabricio Correa","Marcia Yazbell","Xavier Hervas","Mar√≠a Sara Jij√≥n", "Pedro Jos√© Freile","Byron Sol√≠s", "Yaku P√©rez","Washington Pes√°ntez","Jos√© D√≠az","Gustavo Larrea","Alexandra Peralta","Guillermo Lasso","Alfredo Borrero", "Guillermo Celi","Ver√≥nica Sevilla","Juan Fernando Velasco","Ana Mar√≠a Pesantes", "Pa√∫l Carrasco","Frank Vargas Anda", "Ximena Pe√±a","Patricio Barriga","C√©sar Mont√∫far","Julio Villacreses"])
```
## Pol√≠tica por Provincias

### Script 3 (FUENTE 1): Pol√≠ticaxprovincia1.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################
ckey = "CuIFG1DblkEC36JFzJByX5mi7"
csecret = "Nc8sgHQ1n2Fb1qpHma9LudER1wOELLP4tNzzpz6YAIQnmRF9Qh"
atoken = "1204786641635827712-XXmfW6V9O2i6CVS4X1SykT5Tp7GpjY"
asecret = "S76F2rIurjJnpwctfYQ9a2tprXp7pP8UNI8lYlfWSuqKW"

#####################################

class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)


auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:couchy@localhost:5984/')
try:
    db = server.create('porprovincias')
except:
    db = server['porprovincias']

'''===============LISTAS, NOMBRES Y REPRESENTANTES=============='''
twitterStream.filter(track=['Movimiento Auton√≥mico Regional','Teresa Rodas','Jos√© S√°nchez','Carolina √Ålvarez','Arlington M√°rquez','Alba Bravo','Jos√© Felipe Mart√≠nez','G√©nesis Vaca','Mauricio Ortiz',' Rosario Sig√ºenza','Jorge Palacios','Abg. Luis Alberto Morocho Garc√≠a',' Cristina P√©rez C√≥rdova','Jos√© Granda Palomino','Marcela Valencia Renter√≠a',' Fernando Delgado Gordillo','Jaqueline Anz√≥ategui Pel√°ez','Flavio Corozo Castro',' Mar√≠a Cristina Farez','Bryan Erazo Mora','Zulema Veintimilla Castillo','Movimiento Uni√≥n Ecuatoriana','Betzabeth Liliana Herrera Chal√°n','√Ångel Garc√≠a Ram√≥n','Irene Victoria Valdivieso Montero','Jorge Vaca Jervez','Mar√≠a Matilde Suriaga Vicente','Palmer Delgado Echeverr√≠a', 'Digna Emedica Granda Castro','Ayrton Joao Almache Izquierdo', 'Laura Mariela Tinitana Aguilar', 'Miguel √Ångel Vicente Gaona','Movimiento CREO','Francisco Vera Dom√≠nguez', 'Mirtha Aristeguieta Logro√±o','Vicente Guzm√°n Barbot√≥',' Rosa Rom√°n Zambrano','Washington Romero A√±azco',  'Evelyn Escaleras R√≠os','Jos√© Quim√≠ Arias','Lidia Arias Torres','Santiago Romero Granda','Mirelis Tituana Asanza', 'Movimiento unidad popular','Jessica Gonz√°lez Cabrera','Ulbio Torres','Lady Le√≥n','Galo Jim√©nez',' Mar√≠a Isabel Farez','Vicente Naranjo',' Carmen Guerrero','Jonthan Paladines','Maritza Herrera','Bol√≠var Bravo','Movimiento Sur Unido Regional', 'Abg. Galo Suquilanda Jara', 'Abg. Luc√≠a Ramos', 'Lucio Minchala Calder√≥n', 'Silvana Maldonado Pel√°ez','Carlos Guzm√°n Solano','candidatos de Avanza', 'Lista 8', 'Danny Nieto Macas',' Nathaly Teresa Puertas Paladines', 'Wilson Merino S√°nchez', 'Analy Estefan√≠a Carbache Hern√°ndez', 'Jos√© Ayala Chamba', 'Francy Aracely Gonz√°lez Morales', 'Gonzalo Arturo Ortega Pereira', 'Karen Estefan√≠a Beltr√°n Apolo',' Darwin Samuel C√°rdenas Robles','Kerly Taimy Cueva Celi','movimiento Democrac√≠a S√≠', 'Lista 20','Elizabeth Falcon√≠ Niemes', 'Rolando Fernando Ayov√≠ Rodr√≠guez',' Paulette Pulla Carri√≥n', 'Luis Mend√≠a Armijos', 'Julia Mar√≠a Renter√≠a German','Jefferson Ruilova Niemes', 'Alejandra Le√≥n Ord√≥√±ez', 'Segundo Daniel Jumbo Ram√≠rez', 'Normandy Feliza Ayov√≠ Rodr√≠guez','Cristian Ardila Rivera','IZQUIERDA DEMOCR√ÅTICA','Johanna Nicole Moreira C√≥rdova', 'Jes√∫s Alberto Motoche Apolo', 'Valeria Estefan√≠a Elizalde Maza', 'Jaime Hurtado G√°ndara Pizarro', 'Karen Espinoza Castro','Joseph Geovanny Villacreses Quevedo', 'Jenny Requene Bravo', '√Ålvaro Gustavo Ulloa Jaramillo', 'Monica Nathaly Cobos Da√∫l','Mario Eduardo Narv√°ez','movimiento Concertaci√≥n', 'Lista 51','Ver√≥nica Arreaga',' Hugo Juca', 'Mireya Le√≥n', 'Jos√© Ochoa',' Ana Contreras','Marco Torres', 'Lorena Orellana', 'David Ord√≥√±ez', 'B√©lgica Jim√©nez', 'Leonardo Castro', 'Miguel √Ångel Garz√≥n Villacr√©s', 'Adriana Lisbeth Rivilla Ortega', 'Roger Max Medina Espinar', 'Mar√≠a Garc√≠a Dom√≠nguez', 'Gerardo Acu√±a Jara', 'Sisy Alison Armijos Bustamante', 'Gilbert Adri√°n Pont√≥n Jim√©nez', 'Laura Yolanda Naula Rodr√≠guez',' Marlos Onassis Reyes Chamba', 'Ana Mar√≠a Matute Cede√±o','Movimiento Ecuatoriano Unido','Susana Estefan√≠a Sol√≥rzano Astudillo', 'Dorian Dami√°n Flores Aguilera', 'Jazm√≠n Cecibel Cheme Fern√°ndez', 'Luis Alberto Leiva Romero','Mar√≠a Fernanda Carri√≥n Limones','Emilio Efr√©n Cruz Cazares', 'Vanessa Portilla Tituana', 'Juan Carlos Flor Gir√≥n', 'Angie Mishell Yaguachi Camacho', 'Jimmy Alexander A√±azco','PARTIDO SOCIALISTA', 'lista 17 ', 'C√©sar Valarezo Romero', 'Leysi L√≥pez', 'Eudaldo Jad√°n Veri√±az', 'Karla Su√°rez','Fernando Quirola Anzo√°tegui' ,'Carlos Falquez Batallas','alterna Karen Noblecilla','Rosa Mar√≠a Loaiza', 'Enrique Orellana Cueva','Movimiento Alianza Pa√≠s', 'Lista 35', 'Rosa Orellana','Jorge Paredes', 'Cristhian Dumas','alterna Irina Alvarado','Movimiento de Unidad Plurinacional Pachakutik', 'lista 18', 'Darwin Pereira Chamba', 'Mar√≠a Leonila Armijos Yunga', 'Antonio Geovanny Almache Guarderas', 'Nayelhi Mayte Chuchuca Mar√≠n', 'Oscar Aldo S√°nchez Romero', 'Mar√≠a Carre√±o Chamba',' Alejandro Fabricio Romero Espinoza', 'Marl√≠n Anabel Avelino Granda', 'Petter Andr√©s Armijos Yaure','Maritza Amaya Torres','Movimiento SIII', 'Lista 88', 'Aldo Favio Romazzo Guzm√°n', 'Grace Amanda Encalda S√°nchez', 'Mario Farah Rodr√≠guez', 'Dahara Balc√°zar Robles','Cristhian Omar Chalaco Celi','Margarita Aracely Ort√≠z Maza', 'Antony Agenor Le√≥n Espinoza', 'Karin Ernestina Ollague Guzm√°n', 'Juan Francisco Ram√≠rez Loayza','Roxana Nu√±ez Rodr√≠guez','Partido Sociedad Patri√≥tica', 'Marco Antonio Gallardo Su√°rez', 'Carolina Elizabeth Vivanco Cueva',' Carlos Alfredo Loja Sagbay', 'Michelli Zambrano Cruz', 'Rub√©n Eudofilio Zhunio Malla', 'Mar√≠a Jos√© Lom√°s Cerezo', 'Segundo Manuel Balc√°zar Naranjo', 'Mar√≠a Jos√© D√≠az Lude√±a', 'Erick Mauricio Molina Oviedo', 'Indira Priscilla Sacco Freire','Centro Democr√°tico','Lista 1', 'UNES', 'Carlos V√≠ctor Zambrano Land√≠n', 'Mar√≠a Fernanda Astudillo Barrezueta', 'Fernando Javier Guam√°n Andrade', 'Gloria Esterfilia Espinoza V√°squez' , 'Giancarlo Mora Pe√±arreta', 'Sara Noem√≠ Cabrera Chac√≥n', 'Jorge Ariel Quevedo Pinta', 'Diana Ver√≥nica Chuquisala Pinza', 'Kevin Samuel Jim√©nez Barreto','Ariana de Lourdes Pineda Encalada','Jimmy Candell','Lista 63', 'Winston Ajoy','Eduardo Rugel','Ver√≥nica Palma','Bol√≠var Fajardo','Abrahan Segarra','Luis Hemeregildo'])
```

 **Proceso** 
 Como se puede ver, hemos usado el mismo script de python para recolectar tweets sobre este tema. Solo cambian algunas variantes
La secci√≥n ==couchdb== se utiliza la direcci√≥n donde se encuentra corriendo CouchDB, que es la Base de datos NO SQL en la que los datos recopilados se a√±adir√°n. Aqu√≠ le indicamos que se debe crear una base de datos llamada _porprovincias_  
 
Para el filtro de twitter usamos el filtro por palabras debido a que nos permite definir exactamente los temas relacionados a la pol√≠tica. Este apartado contiene aproximadamente 300 palabras como filtro, entre ellos:
 * Nombres de candidatos por provincias
 * Nombres de Movimientos y Listas candidatas
 * Nombres de las provincias
 
Una vez expplicado el script, se procede a correr el mismo.
Para esto nos ubicamos en la carpeta donde se encuentra el script y abrimos la terminal y ejecutamos el comando: **python Politica2.py**



## JUEGOS EN L√çNEA

Primero se detalla el script juegos.py usado para el webscrapping dentro de la p√°gina Steam

### Script 4: juegos.py

```
import scrapy
from scrapy import Spider
from scrapy import Selector
from Steam.items import SteamItem
from scrapy.spiders import CrawlSpider
    
class JuegosSpider(CrawlSpider):
    name = 'juegos'
    allowed_domains = ['steampowered.com']
    start_urls = [
    "https://store.steampowered.com/search/?sort_by=Released_DESC&sort_order=DESC&page=%d" % i for i in range(1,2845)]


    def parse(self, response):
    	games=Selector(response).xpath('//*[@id="search_resultsRows"]/a')
    	for game in games:
    		item=SteamItem()
    		item['nombre']=game.xpath('div[2]/div[1]/span/text()').extract()[0]
    		item['link']=game.xpath('@href').extract()[0]
    		item['fecha']=game.xpath('div[2]/div[2]/text()').extract()[0]
    		item['precio']=game.xpath('div[2]/div[4]/div[2]/text()').extract()[0]
    		yield item
```
**Proceso**
A continuaci√≥n usamos la plataforma de venta de videojuegos  **Steam** 
En esta p√°gina se realiza webscrapping junto con el framework scrapy para lo que luego de tener descargado, lo importamos dentro del script.
El sitio cuenta con 2845 p√°ginas por lo que se utiliz√≥ un for dentro de la URL y as√≠ raspar todo el contenido. 
```
    start_urls = [
    "https://store.steampowered.com/search/?sort_by=Released_DESC&sort_order=DESC&page=%d" % i for i in range(1,2845)]
```
La funci√≥n Selector nos permite obtener los XPath de los componentes dentro de Steam
```
games=Selector(response).xpath('//*[@id="search_resultsRows"]/a')
```
Donde: 

**[@id="search_resultsRows"]/a:** Es el componente principal donde se encuentran los elementos rastrear.
**item['nombre']=game.xpath('div[2]/div[1]/span/text()').extract()[0]:** Indica que se extraer√° el texto de la etiqueta span
**item['link']=game.xpath('@href').extract()[0]:** Se extrae los links dentro de las etiquetas _a_
**item['fecha']=game.xpath('div[2]/div[2]/text()').extract()[0]:** Se extrae las fechas dentro de la etiqueta div
**item['precio']=game.xpath('div[2]/div[4]/div[2]/text()').extract()[0]:** Por ultimo se extraen los precios de los videojuegos que se encuentra en la etiqueta div


**A continuaci√≥n, se extrae datos de twitter relacionados con los videojuegos encontrados dentro de Steam

## Script 5 (FUENTE 1): Juegos1.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################
ckey = "6Zyv4XxVypDqHDpFoHwSTrMzX"
csecret = "3J5TpltHtmEZGEw8RhRLABc3KQ2Quhjj2SVVykfw5zs02fjtpC"
atoken = "153168970-C8H0rPCjztDmLQMrjtgOYSPIzjLMyegrtrAZQQrq"
asecret = "WxWpMOMlghN1tVYZRFugRWTefM1SShLWVI4lL4oPWTAlO"
#####################################
class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)


auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:couchy@localhost:5984/')
try: 
    db = server.create('juegos')

except:
    db = server['juegos']

'''===============LIST OF GAMES=============='''
twitterStream.filter(track=["OkunoKA Madness", "School of Mythology","Gump","No Game No LIFE","Pandemic Bunny","The Rule of Land: Pioneers","The Final Boss Demo","Chickens Madness","World Process","Capital Simulator","Hungry Horace","The Last Show of Mr. Chardish: Demo","Rangok Skies Demo","Bounty Battle","Tamarin","Guild of Darksteel Demo","Microodyssey","AeternoBlade","Knight Arena","Poly Pirates","Alice Sisters","Crown of the Empire","Asian Riddles 2","Sweet Tooth 2","Idle Expanse","Human Diaspora","Beat Flip","Arcanion: Tale of Magi","SpermDash Soundtrack","Sakura Dimensions","Outbreak New Dawn", "Adriatic Pizza","HOLIDAYS","Sunset Shapes","Arabian Treasures: Midnight Match","Immersion Demo","Pro Gymnast","CreepWars TD","Mimicry","FAST & FURIOUS CROSSROADS: Launch Pack","Nightmare Puppeteer Demo","Midnight's Curse","Anime Feet","The Grand Lord","Super Glitter Rush","League of Angels-Heaven's Fury","Star Renegades","RPG Maker MV - Yokai Parade","Sightbringer","Ultimate Wall Defense Force","HYPERBOLIC Arcade Trading","Food Chain","DPS IDLE","Hoops Madness","Squares Rage Soundtrack","Omina Mortis","Pixel Kunoichi"])
```

## Script 5 (FUENTE 2): Juegos2.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################


ckey = "6Zyv4XxVypDqHDpFoHwSTrMzX"
csecret = "3J5TpltHtmEZGEw8RhRLABc3KQ2Quhjj2SVVykfw5zs02fjtpC"
atoken = "153168970-C8H0rPCjztDmLQMrjtgOYSPIzjLMyegrtrAZQQrq"
asecret = "WxWpMOMlghN1tVYZRFugRWTefM1SShLWVI4lL4oPWTAlO"

#####################################

class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)


auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:12345@192.168.1.2:5984/')
try: 
    db = server.create('juegos')

except:
    db = server['juegos']

'''===============LOCATIONS=============='''


twitterStream.filter(track=['Hero Soul: I Want to be a Hero! Demo','Mad Taxi',
'Demolition Expert - The Simulation','The Old House','Clea 2 Demo','REPTOMOM',
'Earth: 9000','TinShift','Artificer: Science of Magic','Rogue Star Rescue Demo','Rubber Toys Demo','Vampire: The Masquerade - Shadows of New York Soundtrack',
'Wildfire Demo'])
```
## Noticia evento mundial: COVID

En este caso se hizo uso de un solo script para extraer los datos
## Script 6 : covid1.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################
ckey = "CuIFG1DblkEC36JFzJByX5mi7"
csecret = "Nc8sgHQ1n2Fb1qpHma9LudER1wOELLP4tNzzpz6YAIQnmRF9Qh"
atoken = "1204786641635827712-XXmfW6V9O2i6CVS4X1SykT5Tp7GpjY"
asecret = "S76F2rIurjJnpwctfYQ9a2tprXp7pP8UNI8lYlfWSuqKW"

#####################################

class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)


auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:couchy@localhost:5984/')
try:
    db = server.create('covid')
except:
    db = server['covid']

'''===============LISTAS, NOMBRES Y REPRESENTANTES=============='''
#twitterStream.filter(locations=[-79.95912,-2.287573,-79.856351,-2.053362]) 
twitterStream.filter(track=["covid-19","COVID","Cuarentena por covid","Latinoam√©rica covid-19","Coronavirus"])
```

## Tema libre: Salida de Messi de Barcelona

En este caso se hizo uso de un solo script para extraer los datos

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################
ckey = "CuIFG1DblkEC36JFzJByX5mi7"
csecret = "Nc8sgHQ1n2Fb1qpHma9LudER1wOELLP4tNzzpz6YAIQnmRF9Qh"
atoken = "1204786641635827712-XXmfW6V9O2i6CVS4X1SykT5Tp7GpjY"
asecret = "S76F2rIurjJnpwctfYQ9a2tprXp7pP8UNI8lYlfWSuqKW"
#####################################

class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)

auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:couchy@localhost:5984/')
try:
    db = server.create('messi')
except:
    db = server['messi']
'''===============LISTAS, NOMBRES Y REPRESENTANTES=============='''
twitterStream.filter(track=["lionel messi","messi","barcelona FC","barca"])
```

## Script 7 : messi1.py


## Transformaci√≥n de datos üìã

En cuanto a la tranformaci√≥n de datos de la base de datos CouchDB, se usaron los siguientes comandos:

## Se tom√≥ los datos de la base de datos _politica2_ que hace referencia a los tweets tomando por ciudades. Se los exporta en archivo JSON

```
curl -X GET http://admin:couchy@127.0.0.1:5984/politica2/_all_docs?include_docs=true > C:/Users/politicaCiudades.json
```

## Se tom√≥ los datos de la base de datos _porprovincias_ que hace referencia a los tweets tomando por provincias. Se los exporta en archivo JSON

```
curl -X GET http://admin:couchy@127.0.0.1:5984/porprovincias/_all_docs?include_docs=true > C:/Users/politicaProvincias.json
```

## Se tom√≥ los datos de la base de datos _covid_ que hace referencia a los tweets tomados acerca de la pandemia del Coronavirus. Se los exporta en archivo JSON

```
curl -X GET http://admin:couchy@127.0.0.1:5984/covid/_all_docs?include_docs=true > C:/Users/covid.json
```

## Se tom√≥ los datos de la base de datos _juegos_ que hace referencia a los tweets tomandos por nombres de juegos encontrados desde el webscrapping. Se los exporta en archivo CSV para su debido an√°lisis

```
curl -X GET http://admin:couchy@127.0.0.1:5984/juegos/_all_docs?include_docs=true > C:/Users/juegosOnline.CSV
```

## Se tom√≥ los datos de la base de datos _messi_ que hace referencia a los tweets sobre la salida del jugador Leo Messi del Barcelona. Se los exporta en archivo JSON

```
curl -X GET http://admin:couchy@127.0.0.1:5984/messi/_all_docs?include_docs=true > C:/Users/messi.json
```

## A continuaci√≥n, para pasar los datos del webscrapping realizado de Steam y el Dataset sobre COVID a la base de datos MYSQL se sigue el siguiente proceso:

## Pasar datos de Dataset tomando de opendataset hacia MYSQL:

**1. Creaci√≥n de tablas dentro de la base de datos covid
```
use covid;
CREATE TABLE canada (
	case_id int,
    province_death_id VARCHAR(45),
    age VARCHAR(45),
    sex VARCHAR(20),
    health_region VARCHAR(45),
    province VARCHAR(45),
    country VARCHAR(45),
    date_death_report date,
    
    
    constraint canadapk PRIMARY KEY (case_id)
);
CREATE TABLE paises_covid (
Id int,	
Pa√≠s varchar(45),	
Frecuencia	varchar(45),
Fecha_de_Inicio	varchar(20),
Fecha_Final varchar (20),	
A√±o	varchar(4),
Mes	int,
Semana	varchar(45),
Muertes	int,
Muertes_Esperadas int,	
Exceso_de_Muertes	int,
Linea_Base varchar(45),
    
    constraint paisespk PRIMARY KEY (Id)
);
```

**2. Uso de comando para importar los datos del archivo Libro1.CSV hacia la base de datos creada

```
load data local infile 'C:/Users/l_jan/Documents/Desarrollo de Software/Cuarto Semestre/Analisis de Datos/Proyecto Final/Covid Dataset/Datos para Power BI/Libro1.csv' into table paises_covid fields terminated by ';' lines terminated by '\r\n';
```
## MapReduce üîß

El MapReduce fue realizado con las herramientas Kibana y PowerBI por lo que se encuentra en la documentaci√≥n.

## Mapping ‚öôÔ∏è

El mapping dentro de cada √≠ndice fueron los siguientes:

Dentro de los √≠ndices: messiBarcelona, politicaxprivincia se usa el script **mapping.conf**

```
{
    "mappings": {
        "properties": {
          "created_at": {
            "type": "date",
            "format": "EE MMM d HH:mm:ss Z yyyy||dd/MM/yyyy||dd-MM-yyyy||date_optional_time"
          },
          "location": {
            "type": "geo_point"
          }
}
```

## Creaci√≥n de √≠ndices (Logstash y Elasticsearch) ‚öôÔ∏è

Una vez extra√≠dos los datos, se procede a pasarlos a Elasticsearch para posterior a ello crear visualizaciones.
Para pasar estos datos se hace uso de la herramietna logstash con los siguientes scripts:

**Script 1: ciudades.conf** 

Se crear√° un √≠ndice dentro de elasticsearch llamado _datos_ el cual contendr√° los documentos de la base de datos _politica2_  de couchDB

```
input{
couchdb_changes{
db=>"poltica2"
} }
output {
 elasticsearch {
 index => "datos"
 
 } 
}
```
**Script 1: politica.conf** 

Se crear√° un √≠ndice dentro de elasticsearch llamado _politicaporprovincia_ el cual contendr√° los documentos de la base de datos _porprovincias_ de couchDB

```
input{
couchdb_changes{
db=>"porprovincias"
} }
output {
 elasticsearch {
 index => "politicaxprovincia"
 
 } 
}
```
**Script 1: messi.conf** 

Se crear√° un √≠ndice dentro de elasticsearch llamado _messibarcelona_ el cual contendr√° los documentos de la base de datos _messi_

```
input{
couchdb_changes{
db=>"messi"
} }
output {
 elasticsearch {
 index => "jmessibarcelona"
 
 } 
}
```

## Autores ‚úíÔ∏è

* **Alejandro Armas** 
* **Yajaira Cuatis** 
* **Christian Llumquinga** 





