# Proyecto de An치lisis de Datos

### Link de One Drive con Datasets, Scripts: https://epnecuador-my.sharepoint.com/personal/jonathan_armas_epn_edu_ec/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fjonathan_armas_epn_edu_ec%2FDocuments%2FDataAnalisis&originalPath=aHR0cHM6Ly9lcG5lY3VhZG9yLW15LnNoYXJlcG9pbnQuY29tLzpmOi9nL3BlcnNvbmFsL2pvbmF0aGFuX2FybWFzX2Vwbl9lZHVfZWMvRXZuNXhTRXBaU1pLdnd5aExpRjN5dlVCd3p0ZVhCRjBjSFJIRW5XQnp4d0VYQT9ydGltZT1wMUJ1aFlsVDJFZw

**archivos JSON y CSV Finales:** https://mega.nz/file/vBEjTSpZ#iZidFwpiJi9ryKcBk8Eymwy0bNcU2eIo6_yLORXcbPg

Proceso detallado sobre el uso de scripts

## 1. Cosecha de datos 游

## Pol칤tica por Ciudades

La referencia _FUENTE_ son los diferentes scripts usados por el equipo, el proceso detallado es similar en los scripts

### Script 1 (FUENTE 1): Politicaxciudad1.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################
ckey = "CuIFG1DblkEC36JFzJByX5mi7"
csecret = "Nc8sgHQ1n2Fb1qpHma9LudER1wOELLP4tNzzpz6YAIQnmRF9Qh"
atoken = "1204786641635827712-XXmfW6V9O2i6CVS4X1SykT5Tp7GpjY"
asecret = "S76F2rIurjJnpwctfYQ9a2tprXp7pP8UNI8lYlfWSuqKW"
#####################################

class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)

auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:couchy@localhost:5984/')
try:
    db = server.create('politica2')
except:
    db = server['politica2']
'''===============LISTAS, NOMBRES Y REPRESENTANTES=============='''
twitterStream.filter(track=['Quito candidatos','Ecuador','Machala','Portoviejo','Esmeraldas','Ciudad de Milagro','Babahoyo','Latacunga','Guillermo Alberto Santiago Lasso Mendoza','Fabricio Correa Delgado','Wilson Gustavo Larrea Cabrera','Lucio Guti칠rrez Borb칰a','Andr칠s David Ar치uz Galarza','Guillermo Alejandro Celi Santos','Yaku Sacha P칠rez Guartambel','Cesar Mont칰far Mancheno','Isidro Romero Carbo','Carlos Gerson Almedia Espinoza','Ximena Pe침a Pacheco','Pa칰l Ernesto Carrasco Carpio','Esteban Leopoldo Quirola Bustos','Miguel Salem Kronfle','Cristina Reyes','Xavier Hervas','Pedro Jos칠 Freile','Juan Fernando Velasco Torres','Washington Arturo Pes치ntez Mu침oz','Uni칩n Ecuatoriana','Pachakutik','Partido Social Patriota','Consejo Nacional Electoral Ecuador','Centro democr치tico','Pierina Correa Delgado','Geovanni Atarihuana','Uni칩n Popular','Ecuador unido','Roc칤o Juca', 'Partido Social Cristiano','Henry Kronfle', 'Javier Ortiz', 'Libertad es Pueblo','Fernando Balda','Fuerza EC','Abdal치 Bucaram Ortiz', 'Democracia S칤','Xavier Zavala Egas','C칠sar Monge','Alianza PAIS',  'C칠sar Litardo','Fernando Villavicencio'])

```

**Proceso** 
 En este caso se importaron librer칤as necesarias como twepy el cual nos permite recolectar los tweets en tiempo  real dentro de scripts de python. 
 Tenemos adem치s las API KEY generadas desde Twitter Development las cuales nos permiten acceder a la API de twitter para extraer los datos.
 A continuaci칩n la secci칩n ==couchdb== se utiliza la direcci칩n donde se encuentra corriendo CouchDB, que es la Base de datos NO SQL en la que los datos recopilados se a침adir치n. Aqu칤 le indicamos que se debe crear una base de datos llamada _politica2_  
 
Para el filtro de twitter usamos el filtro por palabras debido a que nos permite definir exactamente los temas relacionados a la pol칤tica. Este apartado contiene aproximadamente 100 palabras como filtro, entre ellos:
 * Nombres de candidatos de las ciudades
 * Nombres de Movimientos y Listas candidatas
 * Nombres de las Ciudades 
 
Una vez explicado el script, se procede a correr el mismo.
Para esto nos ubicamos en la carpeta donde se encuentra el script y abrimos la terminal y ejecutamos el comando: **python Politica1.py**

### Script 2 (FUENTE 2): Pol칤ticaxciudad2.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json


###API ########################
ckey = "6Zyv4XxVypDqHDpFoHwSTrMzX"
csecret = "3J5TpltHtmEZGEw8RhRLABc3KQ2Quhjj2SVVykfw5zs02fjtpC"
atoken = "153168970-C8H0rPCjztDmLQMrjtgOYSPIzjLMyegrtrAZQQrq"
asecret = "WxWpMOMlghN1tVYZRFugRWTefM1SShLWVI4lL4oPWTAlO"
#####################################

class listener(StreamListener):
    
    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print ("SAVED" + str(doc) +"=>" + str(data))
        except:
            print ("Already exists")
            pass
        return True
    
    def on_error(self, status):
        print (status)
        
auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:admin@localhost:5984/')  #('http://115.146.93.184:5984/')
try:
    db = server.create('presidenciales_nacionales')
except:
    db = server['presidenciales_nacionales']
    
'''===============LOCATIONS=============='''    

#twitterStream.filter(locations=[-79.95912,-2.287573,-79.856351,-2.053362]) 
twitterStream.filter(track=["Andr칠s Arauz","Lucio Guti칠rrez","David Norero","Gerson Almeida","Martha Villafuerte","Cristina Reyes","Diego Salgado","Isidro Romero","Sof칤a Merino", "Esteban Quirola", "Juan Carlos Machuca","Miguel Salem Kronfle","Gustavo Bucaram Ortiz", "Fabricio Correa","Marcia Yazbell","Xavier Hervas","Mar칤a Sara Jij칩n", "Pedro Jos칠 Freile","Byron Sol칤s", "Yaku P칠rez","Washington Pes치ntez","Jos칠 D칤az","Gustavo Larrea","Alexandra Peralta","Guillermo Lasso","Alfredo Borrero", "Guillermo Celi","Ver칩nica Sevilla","Juan Fernando Velasco","Ana Mar칤a Pesantes", "Pa칰l Carrasco","Frank Vargas Anda", "Ximena Pe침a","Patricio Barriga","C칠sar Mont칰far","Julio Villacreses"])
```
## Pol칤tica por Provincias

### Script 3 (FUENTE 1): Pol칤ticaxprovincia1.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################
ckey = "CuIFG1DblkEC36JFzJByX5mi7"
csecret = "Nc8sgHQ1n2Fb1qpHma9LudER1wOELLP4tNzzpz6YAIQnmRF9Qh"
atoken = "1204786641635827712-XXmfW6V9O2i6CVS4X1SykT5Tp7GpjY"
asecret = "S76F2rIurjJnpwctfYQ9a2tprXp7pP8UNI8lYlfWSuqKW"

#####################################

class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)


auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:couchy@localhost:5984/')
try:
    db = server.create('porprovincias')
except:
    db = server['porprovincias']

'''===============LISTAS, NOMBRES Y REPRESENTANTES=============='''
twitterStream.filter(track=['Movimiento Auton칩mico Regional','Teresa Rodas','Jos칠 S치nchez','Carolina 츼lvarez','Arlington M치rquez','Alba Bravo','Jos칠 Felipe Mart칤nez','G칠nesis Vaca','Mauricio Ortiz',' Rosario Sig칲enza','Jorge Palacios','Abg. Luis Alberto Morocho Garc칤a',' Cristina P칠rez C칩rdova','Jos칠 Granda Palomino','Marcela Valencia Renter칤a',' Fernando Delgado Gordillo','Jaqueline Anz칩ategui Pel치ez','Flavio Corozo Castro',' Mar칤a Cristina Farez','Bryan Erazo Mora','Zulema Veintimilla Castillo','Movimiento Uni칩n Ecuatoriana','Betzabeth Liliana Herrera Chal치n','츼ngel Garc칤a Ram칩n','Irene Victoria Valdivieso Montero','Jorge Vaca Jervez','Mar칤a Matilde Suriaga Vicente','Palmer Delgado Echeverr칤a', 'Digna Emedica Granda Castro','Ayrton Joao Almache Izquierdo', 'Laura Mariela Tinitana Aguilar', 'Miguel 츼ngel Vicente Gaona','Movimiento CREO','Francisco Vera Dom칤nguez', 'Mirtha Aristeguieta Logro침o','Vicente Guzm치n Barbot칩',' Rosa Rom치n Zambrano','Washington Romero A침azco',  'Evelyn Escaleras R칤os','Jos칠 Quim칤 Arias','Lidia Arias Torres','Santiago Romero Granda','Mirelis Tituana Asanza', 'Movimiento unidad popular','Jessica Gonz치lez Cabrera','Ulbio Torres','Lady Le칩n','Galo Jim칠nez',' Mar칤a Isabel Farez','Vicente Naranjo',' Carmen Guerrero','Jonthan Paladines','Maritza Herrera','Bol칤var Bravo','Movimiento Sur Unido Regional', 'Abg. Galo Suquilanda Jara', 'Abg. Luc칤a Ramos', 'Lucio Minchala Calder칩n', 'Silvana Maldonado Pel치ez','Carlos Guzm치n Solano','candidatos de Avanza', 'Lista 8', 'Danny Nieto Macas',' Nathaly Teresa Puertas Paladines', 'Wilson Merino S치nchez', 'Analy Estefan칤a Carbache Hern치ndez', 'Jos칠 Ayala Chamba', 'Francy Aracely Gonz치lez Morales', 'Gonzalo Arturo Ortega Pereira', 'Karen Estefan칤a Beltr치n Apolo',' Darwin Samuel C치rdenas Robles','Kerly Taimy Cueva Celi','movimiento Democrac칤a S칤', 'Lista 20','Elizabeth Falcon칤 Niemes', 'Rolando Fernando Ayov칤 Rodr칤guez',' Paulette Pulla Carri칩n', 'Luis Mend칤a Armijos', 'Julia Mar칤a Renter칤a German','Jefferson Ruilova Niemes', 'Alejandra Le칩n Ord칩침ez', 'Segundo Daniel Jumbo Ram칤rez', 'Normandy Feliza Ayov칤 Rodr칤guez','Cristian Ardila Rivera','IZQUIERDA DEMOCR츼TICA','Johanna Nicole Moreira C칩rdova', 'Jes칰s Alberto Motoche Apolo', 'Valeria Estefan칤a Elizalde Maza', 'Jaime Hurtado G치ndara Pizarro', 'Karen Espinoza Castro','Joseph Geovanny Villacreses Quevedo', 'Jenny Requene Bravo', '츼lvaro Gustavo Ulloa Jaramillo', 'Monica Nathaly Cobos Da칰l','Mario Eduardo Narv치ez','movimiento Concertaci칩n', 'Lista 51','Ver칩nica Arreaga',' Hugo Juca', 'Mireya Le칩n', 'Jos칠 Ochoa',' Ana Contreras','Marco Torres', 'Lorena Orellana', 'David Ord칩침ez', 'B칠lgica Jim칠nez', 'Leonardo Castro', 'Miguel 츼ngel Garz칩n Villacr칠s', 'Adriana Lisbeth Rivilla Ortega', 'Roger Max Medina Espinar', 'Mar칤a Garc칤a Dom칤nguez', 'Gerardo Acu침a Jara', 'Sisy Alison Armijos Bustamante', 'Gilbert Adri치n Pont칩n Jim칠nez', 'Laura Yolanda Naula Rodr칤guez',' Marlos Onassis Reyes Chamba', 'Ana Mar칤a Matute Cede침o','Movimiento Ecuatoriano Unido','Susana Estefan칤a Sol칩rzano Astudillo', 'Dorian Dami치n Flores Aguilera', 'Jazm칤n Cecibel Cheme Fern치ndez', 'Luis Alberto Leiva Romero','Mar칤a Fernanda Carri칩n Limones','Emilio Efr칠n Cruz Cazares', 'Vanessa Portilla Tituana', 'Juan Carlos Flor Gir칩n', 'Angie Mishell Yaguachi Camacho', 'Jimmy Alexander A침azco','PARTIDO SOCIALISTA', 'lista 17 ', 'C칠sar Valarezo Romero', 'Leysi L칩pez', 'Eudaldo Jad치n Veri침az', 'Karla Su치rez','Fernando Quirola Anzo치tegui' ,'Carlos Falquez Batallas','alterna Karen Noblecilla','Rosa Mar칤a Loaiza', 'Enrique Orellana Cueva','Movimiento Alianza Pa칤s', 'Lista 35', 'Rosa Orellana','Jorge Paredes', 'Cristhian Dumas','alterna Irina Alvarado','Movimiento de Unidad Plurinacional Pachakutik', 'lista 18', 'Darwin Pereira Chamba', 'Mar칤a Leonila Armijos Yunga', 'Antonio Geovanny Almache Guarderas', 'Nayelhi Mayte Chuchuca Mar칤n', 'Oscar Aldo S치nchez Romero', 'Mar칤a Carre침o Chamba',' Alejandro Fabricio Romero Espinoza', 'Marl칤n Anabel Avelino Granda', 'Petter Andr칠s Armijos Yaure','Maritza Amaya Torres','Movimiento SIII', 'Lista 88', 'Aldo Favio Romazzo Guzm치n', 'Grace Amanda Encalda S치nchez', 'Mario Farah Rodr칤guez', 'Dahara Balc치zar Robles','Cristhian Omar Chalaco Celi','Margarita Aracely Ort칤z Maza', 'Antony Agenor Le칩n Espinoza', 'Karin Ernestina Ollague Guzm치n', 'Juan Francisco Ram칤rez Loayza','Roxana Nu침ez Rodr칤guez','Partido Sociedad Patri칩tica', 'Marco Antonio Gallardo Su치rez', 'Carolina Elizabeth Vivanco Cueva',' Carlos Alfredo Loja Sagbay', 'Michelli Zambrano Cruz', 'Rub칠n Eudofilio Zhunio Malla', 'Mar칤a Jos칠 Lom치s Cerezo', 'Segundo Manuel Balc치zar Naranjo', 'Mar칤a Jos칠 D칤az Lude침a', 'Erick Mauricio Molina Oviedo', 'Indira Priscilla Sacco Freire','Centro Democr치tico','Lista 1', 'UNES', 'Carlos V칤ctor Zambrano Land칤n', 'Mar칤a Fernanda Astudillo Barrezueta', 'Fernando Javier Guam치n Andrade', 'Gloria Esterfilia Espinoza V치squez' , 'Giancarlo Mora Pe침arreta', 'Sara Noem칤 Cabrera Chac칩n', 'Jorge Ariel Quevedo Pinta', 'Diana Ver칩nica Chuquisala Pinza', 'Kevin Samuel Jim칠nez Barreto','Ariana de Lourdes Pineda Encalada','Jimmy Candell','Lista 63', 'Winston Ajoy','Eduardo Rugel','Ver칩nica Palma','Bol칤var Fajardo','Abrahan Segarra','Luis Hemeregildo'])
```

 **Proceso** 
 Como se puede ver, hemos usado el mismo script de python para recolectar tweets sobre este tema. Solo cambian algunas variantes
La secci칩n ==couchdb== se utiliza la direcci칩n donde se encuentra corriendo CouchDB, que es la Base de datos NO SQL en la que los datos recopilados se a침adir치n. Aqu칤 le indicamos que se debe crear una base de datos llamada _porprovincias_  
 
Para el filtro de twitter usamos el filtro por palabras debido a que nos permite definir exactamente los temas relacionados a la pol칤tica. Este apartado contiene aproximadamente 300 palabras como filtro, entre ellos:
 * Nombres de candidatos por provincias
 * Nombres de Movimientos y Listas candidatas
 * Nombres de las provincias
 
Una vez expplicado el script, se procede a correr el mismo.
Para esto nos ubicamos en la carpeta donde se encuentra el script y abrimos la terminal y ejecutamos el comando: **python Politica2.py**



## JUEGOS EN L칈NEA

Primero se detalla el script juegos.py usado para el webscrapping dentro de la p치gina Steam

### Script 4: juegos.py

```
import scrapy
from scrapy import Spider
from scrapy import Selector
from Steam.items import SteamItem
from scrapy.spiders import CrawlSpider
    
class JuegosSpider(CrawlSpider):
    name = 'juegos'
    allowed_domains = ['steampowered.com']
    start_urls = [
    "https://store.steampowered.com/search/?sort_by=Released_DESC&sort_order=DESC&page=%d" % i for i in range(1,2845)]


    def parse(self, response):
    	games=Selector(response).xpath('//*[@id="search_resultsRows"]/a')
    	for game in games:
    		item=SteamItem()
    		item['nombre']=game.xpath('div[2]/div[1]/span/text()').extract()[0]
    		item['link']=game.xpath('@href').extract()[0]
    		item['fecha']=game.xpath('div[2]/div[2]/text()').extract()[0]
    		item['precio']=game.xpath('div[2]/div[4]/div[2]/text()').extract()[0]
    		yield item
```
**Proceso**
A continuaci칩n usamos la plataforma de venta de videojuegos  **Steam** 
En esta p치gina se realiza webscrapping junto con el framework scrapy para lo que luego de tener descargado, lo importamos dentro del script.
El sitio cuenta con 2845 p치ginas por lo que se utiliz칩 un for dentro de la URL y as칤 raspar todo el contenido. 
```
    start_urls = [
    "https://store.steampowered.com/search/?sort_by=Released_DESC&sort_order=DESC&page=%d" % i for i in range(1,2845)]
```
La funci칩n Selector nos permite obtener los XPath de los componentes dentro de Steam
```
games=Selector(response).xpath('//*[@id="search_resultsRows"]/a')
```
Donde: 

**[@id="search_resultsRows"]/a:** Es el componente principal donde se encuentran los elementos rastrear.
**item['nombre']=game.xpath('div[2]/div[1]/span/text()').extract()[0]:** Indica que se extraer치 el texto de la etiqueta span
**item['link']=game.xpath('@href').extract()[0]:** Se extrae los links dentro de las etiquetas _a_
**item['fecha']=game.xpath('div[2]/div[2]/text()').extract()[0]:** Se extrae las fechas dentro de la etiqueta div
**item['precio']=game.xpath('div[2]/div[4]/div[2]/text()').extract()[0]:** Por ultimo se extraen los precios de los videojuegos que se encuentra en la etiqueta div


**A continuaci칩n, se extrae datos de twitter relacionados con los videojuegos encontrados dentro de Steam

## Script 5 (FUENTE 1): Juegos1.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################
ckey = "6Zyv4XxVypDqHDpFoHwSTrMzX"
csecret = "3J5TpltHtmEZGEw8RhRLABc3KQ2Quhjj2SVVykfw5zs02fjtpC"
atoken = "153168970-C8H0rPCjztDmLQMrjtgOYSPIzjLMyegrtrAZQQrq"
asecret = "WxWpMOMlghN1tVYZRFugRWTefM1SShLWVI4lL4oPWTAlO"
#####################################
class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)


auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:couchy@localhost:5984/')
try: 
    db = server.create('juegos')

except:
    db = server['juegos']

'''===============LIST OF GAMES=============='''
twitterStream.filter(track=["OkunoKA Madness", "School of Mythology","Gump","No Game No LIFE","Pandemic Bunny","The Rule of Land: Pioneers","The Final Boss Demo","Chickens Madness","World Process","Capital Simulator","Hungry Horace","The Last Show of Mr. Chardish: Demo","Rangok Skies Demo","Bounty Battle","Tamarin","Guild of Darksteel Demo","Microodyssey","AeternoBlade","Knight Arena","Poly Pirates","Alice Sisters","Crown of the Empire","Asian Riddles 2","Sweet Tooth 2","Idle Expanse","Human Diaspora","Beat Flip","Arcanion: Tale of Magi","SpermDash Soundtrack","Sakura Dimensions","Outbreak New Dawn", "Adriatic Pizza","HOLIDAYS","Sunset Shapes","Arabian Treasures: Midnight Match","Immersion Demo","Pro Gymnast","CreepWars TD","Mimicry","FAST & FURIOUS CROSSROADS: Launch Pack","Nightmare Puppeteer Demo","Midnight's Curse","Anime Feet","The Grand Lord","Super Glitter Rush","League of Angels-Heaven's Fury","Star Renegades","RPG Maker MV - Yokai Parade","Sightbringer","Ultimate Wall Defense Force","HYPERBOLIC Arcade Trading","Food Chain","DPS IDLE","Hoops Madness","Squares Rage Soundtrack","Omina Mortis","Pixel Kunoichi"])
```

## Script 5 (FUENTE 2): Juegos2.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################


ckey = "6Zyv4XxVypDqHDpFoHwSTrMzX"
csecret = "3J5TpltHtmEZGEw8RhRLABc3KQ2Quhjj2SVVykfw5zs02fjtpC"
atoken = "153168970-C8H0rPCjztDmLQMrjtgOYSPIzjLMyegrtrAZQQrq"
asecret = "WxWpMOMlghN1tVYZRFugRWTefM1SShLWVI4lL4oPWTAlO"

#####################################

class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)


auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:12345@192.168.1.2:5984/')
try: 
    db = server.create('juegos')

except:
    db = server['juegos']

'''===============LOCATIONS=============='''


twitterStream.filter(track=['Hero Soul: I Want to be a Hero! Demo','Mad Taxi',
'Demolition Expert - The Simulation','The Old House','Clea 2 Demo','REPTOMOM',
'Earth: 9000','TinShift','Artificer: Science of Magic','Rogue Star Rescue Demo','Rubber Toys Demo','Vampire: The Masquerade - Shadows of New York Soundtrack',
'Wildfire Demo'])
```
## Noticia evento mundial: COVID

En este caso se hizo uso de un solo script para extraer los datos
## Script 6 : covid1.py

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################
ckey = "CuIFG1DblkEC36JFzJByX5mi7"
csecret = "Nc8sgHQ1n2Fb1qpHma9LudER1wOELLP4tNzzpz6YAIQnmRF9Qh"
atoken = "1204786641635827712-XXmfW6V9O2i6CVS4X1SykT5Tp7GpjY"
asecret = "S76F2rIurjJnpwctfYQ9a2tprXp7pP8UNI8lYlfWSuqKW"

#####################################

class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)


auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:couchy@localhost:5984/')
try:
    db = server.create('covid')
except:
    db = server['covid']

'''===============LISTAS, NOMBRES Y REPRESENTANTES=============='''
#twitterStream.filter(locations=[-79.95912,-2.287573,-79.856351,-2.053362]) 
twitterStream.filter(track=["covid-19","COVID","Cuarentena por covid","Latinoam칠rica covid-19","Coronavirus"])
```

## Tema libre: Salida de Messi de Barcelona

En este caso se hizo uso de un solo script para extraer los datos

```
import couchdb
from tweepy import Stream
from tweepy import OAuthHandler
from tweepy.streaming import StreamListener
import json

###API ########################
ckey = "CuIFG1DblkEC36JFzJByX5mi7"
csecret = "Nc8sgHQ1n2Fb1qpHma9LudER1wOELLP4tNzzpz6YAIQnmRF9Qh"
atoken = "1204786641635827712-XXmfW6V9O2i6CVS4X1SykT5Tp7GpjY"
asecret = "S76F2rIurjJnpwctfYQ9a2tprXp7pP8UNI8lYlfWSuqKW"
#####################################

class listener(StreamListener):

    def on_data(self, data):
        dictTweet = json.loads(data)
        try:
            dictTweet["_id"] = str(dictTweet['id'])
            doc = db.save(dictTweet)
            print("SAVED" + str(doc) + "=>" + str(data))
        except:
            print("Already exists")
            pass
        return True

    def on_error(self, status):
        print(status)

auth = OAuthHandler(ckey, csecret)
auth.set_access_token(atoken, asecret)
twitterStream = Stream(auth, listener())

'''========couchdb'=========='''
server = couchdb.Server('http://admin:couchy@localhost:5984/')
try:
    db = server.create('messi')
except:
    db = server['messi']
'''===============LISTAS, NOMBRES Y REPRESENTANTES=============='''
twitterStream.filter(track=["lionel messi","messi","barcelona FC","barca"])
```

## Script 7 : messi1.py


## Transformaci칩n de datos 游늶

En cuanto a la tranformaci칩n de datos de la base de datos CouchDB, se usaron los siguientes comandos:

## Se tom칩 los datos de la base de datos _politica2_ que hace referencia a los tweets tomando por ciudades. Se los exporta en archivo JSON

```
curl -X GET http://admin:couchy@127.0.0.1:5984/politica2/_all_docs?include_docs=true > C:/Users/politicaCiudades.json
```

## Se tom칩 los datos de la base de datos _porprovincias_ que hace referencia a los tweets tomando por provincias. Se los exporta en archivo JSON

```
curl -X GET http://admin:couchy@127.0.0.1:5984/porprovincias/_all_docs?include_docs=true > C:/Users/politicaProvincias.json
```

## Se tom칩 los datos de la base de datos _covid_ que hace referencia a los tweets tomados acerca de la pandemia del Coronavirus. Se los exporta en archivo JSON

```
curl -X GET http://admin:couchy@127.0.0.1:5984/covid/_all_docs?include_docs=true > C:/Users/covid.json
```

## Se tom칩 los datos de la base de datos _juegos_ que hace referencia a los tweets tomandos por nombres de juegos encontrados desde el webscrapping. Se los exporta en archivo CSV para su debido an치lisis

```
curl -X GET http://admin:couchy@127.0.0.1:5984/juegos/_all_docs?include_docs=true > C:/Users/juegosOnline.CSV
```

## Se tom칩 los datos de la base de datos _messi_ que hace referencia a los tweets sobre la salida del jugador Leo Messi del Barcelona. Se los exporta en archivo JSON

```
curl -X GET http://admin:couchy@127.0.0.1:5984/messi/_all_docs?include_docs=true > C:/Users/messi.json
```

## A continuaci칩n, para pasar los datos del webscrapping realizado de Steam y el Dataset sobre COVID a la base de datos MYSQL se sigue el siguiente proceso:

## Pasar datos de Dataset tomando de opendataset hacia MYSQL:

**1. Creaci칩n de tablas dentro de la base de datos covid
```
use covid;
CREATE TABLE canada (
	case_id int,
    province_death_id VARCHAR(45),
    age VARCHAR(45),
    sex VARCHAR(20),
    health_region VARCHAR(45),
    province VARCHAR(45),
    country VARCHAR(45),
    date_death_report date,
    
    
    constraint canadapk PRIMARY KEY (case_id)
);
CREATE TABLE paises_covid (
Id int,	
Pa칤s varchar(45),	
Frecuencia	varchar(45),
Fecha_de_Inicio	varchar(20),
Fecha_Final varchar (20),	
A침o	varchar(4),
Mes	int,
Semana	varchar(45),
Muertes	int,
Muertes_Esperadas int,	
Exceso_de_Muertes	int,
Linea_Base varchar(45),
    
    constraint paisespk PRIMARY KEY (Id)
);
```

**2. Uso de comando para importar los datos del archivo Libro1.CSV hacia la base de datos creada

```
load data local infile 'C:/Users/l_jan/Documents/Desarrollo de Software/Cuarto Semestre/Analisis de Datos/Proyecto Final/Covid Dataset/Datos para Power BI/Libro1.csv' into table paises_covid fields terminated by ';' lines terminated by '\r\n';
```
## MapReduce 游댢

El MapReduce fue realizado con las herramientas Kibana y PowerBI por lo que se encuentra en la documentaci칩n.

## Mapping 丘뙖잺

El mapping dentro de cada 칤ndice fueron los siguientes:

Dentro de los 칤ndices: messiBarcelona, politicaxprivincia se usa el script **mapping.conf**

```
{
    "mappings": {
        "properties": {
          "created_at": {
            "type": "date",
            "format": "EE MMM d HH:mm:ss Z yyyy||dd/MM/yyyy||dd-MM-yyyy||date_optional_time"
          },
          "location": {
            "type": "geo_point"
          }
}
```

## Creaci칩n de 칤ndices (Logstash y Elasticsearch) 丘뙖잺

Una vez extra칤dos los datos, se procede a pasarlos a Elasticsearch para posterior a ello crear visualizaciones.
Para pasar estos datos se hace uso de la herramietna logstash con los siguientes scripts:

**Script 1: ciudades.conf** 

Se crear치 un 칤ndice dentro de elasticsearch llamado _datos_ el cual contendr치 los documentos de la base de datos _politica2_  de couchDB

```
input{
couchdb_changes{
db=>"poltica2"
} }
output {
 elasticsearch {
 index => "datos"
 
 } 
}
```
**Script 1: politica.conf** 

Se crear치 un 칤ndice dentro de elasticsearch llamado _politicaporprovincia_ el cual contendr치 los documentos de la base de datos _porprovincias_ de couchDB

```
input{
couchdb_changes{
db=>"porprovincias"
} }
output {
 elasticsearch {
 index => "politicaxprovincia"
 
 } 
}
```
**Script 1: messi.conf** 

Se crear치 un 칤ndice dentro de elasticsearch llamado _messibarcelona_ el cual contendr치 los documentos de la base de datos _messi_

```
input{
couchdb_changes{
db=>"messi"
} }
output {
 elasticsearch {
 index => "jmessibarcelona"
 
 } 
}
```

## Autores 九뉦잺

* **Alejandro Armas** 
* **Yajaira Cuatis** 
* **Christian Llumquinga** 





